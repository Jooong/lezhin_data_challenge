{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_purchase = pd.read_csv(\"./clean/X_purchase.csv\",index_col=\"Unnamed: 0\")\n",
    "X_non_purchase = pd.read_csv(\"./clean/X_non_purchase.csv\",index_col=\"Unnamed: 0\")\n",
    "y_purchase = pd.read_csv(\"./clean/y_purchase.csv\",header=None,index_col=0)\n",
    "y_non_purchase = pd.read_csv(\"./clean/y_non_purchase.csv\",header=None,index_col=0)\n",
    "\n",
    "X_val_purchase = pd.read_csv(\"./clean/X_val_purchase.csv\",index_col=\"Unnamed: 0\")\n",
    "X_val_non_purchase = pd.read_csv(\"./clean/X_val_non_purchase.csv\",index_col=\"Unnamed: 0\")\n",
    "y_val_purchase = pd.read_csv(\"./clean/y_val_purchase.csv\",header=None,index_col=0)\n",
    "y_val_non_purchase = pd.read_csv(\"./clean/y_val_non_purchase.csv\",header=None,index_col=0)\n",
    "\n",
    "X_test_purchase = pd.read_csv(\"./clean/X_test_purchase.csv\",index_col=\"Unnamed: 0\")\n",
    "X_test_non_purchase = pd.read_csv(\"./clean/X_test_non_purchase.csv\",index_col=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input_purchase = X_purchase.shape[1]\n",
    "n_input_non_purchase = X_non_purchase.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_purchase, 256),\n",
    "            \n",
    "            nn.Linear(256, 512),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, 512),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(512),  \n",
    "            \n",
    "            nn.Linear(512, 256),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(256),  \n",
    "            \n",
    "            nn.Linear(256, 64),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Linear(64, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_purchase, 512),\n",
    "    \n",
    "            nn.Linear(512, 1024),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, 512),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(512),  \n",
    "            \n",
    "            nn.Linear(512, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ver3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_purchase, 512),\n",
    "\n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(512),  \n",
    "            \n",
    "            nn.Linear(512, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ver4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_purchase, 256),\n",
    "            \n",
    "            nn.Linear(256, 512),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 1024),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 512),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 64),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(64, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ver1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_non_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_non_purchase, 128),\n",
    "\n",
    "            nn.Linear(128, 256),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 256),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(256),  \n",
    "            \n",
    "            nn.Linear(256, 32),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(32, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_non_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_non_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input_non_purchase, 128),\n",
    "\n",
    "            nn.Linear(128, 256),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, 512),             \n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            nn.BatchNorm1d(512),\n",
    "\n",
    "            nn.Linear(512, 256),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(256),  \n",
    "            \n",
    "            nn.Linear(256, 32),                 \n",
    "            nn.LeakyReLU(0.2, inplace=True),      \n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(32, 1),                 \n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), n_input_non_purchase)                \n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1 = MLP_purchase().cuda()\n",
    "mlp_2 = MLP_non_purchase().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init(mlp_1)\n",
    "weights_init(mlp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate는 학습 중간중간 변경했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().cuda()   \n",
    "lr_1 = 0.2\n",
    "lr_2 = 0.002\n",
    "optimizer1 = torch.optim.Adam(mlp_1.parameters(), lr=lr_1)\n",
    "optimizer2 = torch.optim.Adam(mlp_2.parameters(), lr=lr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization을 도입했었기 때문에 배치 사이즈도 조정해봄.\n",
    "\n",
    "결과적으론 크면 클 수록 validation score가 높았음.\n",
    "\n",
    "Pytorch에서 모델에 input으로 넣을 수 있는 tensor 크기에 제한이 있기에 미니배치를 두지 않으면 non-contiguous 에러 발생\n",
    "\n",
    "에러 발생하지 않는 가장 큰 크기인 10000 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "NUM_BATCH = X_purchase.shape[0] // BATCH_SIZE\n",
    "\n",
    "X_purchase_np = np.array(X_purchase)\n",
    "y_purchase_np = np.array(y_purchase)\n",
    "\n",
    "X_batches_p = np.array([X_purchase_np[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE] for i in range(NUM_BATCH)])\n",
    "y_batches_p = np.array([y_purchase_np[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE] for i in range(NUM_BATCH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "NUM_BATCH = X_non_purchase.shape[0] // BATCH_SIZE\n",
    "\n",
    "X_non_purchase_np = np.array(X_non_purchase)\n",
    "y_non_purchase_np = np.array(y_non_purchase)\n",
    "\n",
    "X_batches_n = np.array([X_non_purchase_np[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE] for i in range(NUM_BATCH)])\n",
    "y_batches_n = np.array([y_non_purchase_np[i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE] for i in range(NUM_BATCH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_purchase_v = Variable(torch.FloatTensor(np.array(X_val_purchase).astype(float)).cuda())\n",
    "y_val_purchase_v = Variable(torch.FloatTensor(np.array(y_val_purchase).astype(float)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val_non_v = Variable(torch.FloatTensor(np.array(X_val_non_purchase).astype(float)).cuda())\n",
    "y_val_non_v = Variable(torch.FloatTensor(np.array(y_val_non_purchase).astype(float)).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train on  purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses_p = []\n",
    "train_scores_p = []\n",
    "val_scores_p = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8fbcccd4cb90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmlp_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred_train_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbackend_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/thnn/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resize_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unresize_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[0;32m---> 41\u001b[0;31m                                                    output, *self.additional_args)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for n, (X_batch,y_batch) in enumerate(zip(X_batches_p,y_batches_p)):\n",
    "        X_batch_v = Variable(torch.FloatTensor(X_batch).cuda()) \n",
    "        labels = Variable(torch.FloatTensor(y_batch.astype(float)).cuda())\n",
    "\n",
    "        mlp_1.zero_grad()                                 \n",
    "        y_pred_train_v = mlp_1(X_batch_v)\n",
    "        train_loss = criterion(y_pred_train_v, labels)\n",
    "\n",
    "        train_loss.backward()                                     \n",
    "        optimizer1.step()\n",
    "\n",
    "        \n",
    "    y_pred_train = y_pred_train_v.data.cpu().numpy().flatten()\n",
    "    train_score = roc_auc_score(y_batch,y_pred_train)\n",
    "\n",
    "    y_pred = mlp_1(X_val_purchase_v).data.cpu().numpy().flatten()\n",
    "    y_true = y_val_purchase\n",
    "    \n",
    "    val_score = roc_auc_score(y_true,y_pred)\n",
    "    \n",
    "    print(\"epoch: {},loss: {}, train_score:{},val_score: {}\".format(epoch+1,train_loss.data[0],train_score,val_score))\n",
    "\n",
    "    train_scores_p.append(train_score)\n",
    "    train_losses_p.append(train_loss.data[0])\n",
    "    val_scores_p.append(val_score)\n",
    "    if val_score > 0.8684:\n",
    "        torch.save(mlp_1.state_dict(), 'mlp_purchase_{}.pth'.format(epoch))\n",
    "        pickle.dump(train_scores_p,open(\"train_scores_p.p\",\"wb\"))\n",
    "        pickle.dump(train_losses_p,open(\"train_losses_p.p\",\"wb\"))\n",
    "        pickle.dump(val_scores_p,open(\"val_scores_p.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[purchase]\n",
    "\n",
    "ver1 with batch 100000\n",
    "\n",
    "epoch: 16,loss: 0.34876522421836853, train_score:0.9047321739061429,val_score: 0.8684095545793991\n",
    "\n",
    "-> 이게 제일 잘 나옴. 모델을 더 deep하게 만들거나 얕게 만들어도 이 validation 스코어를 갱신하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses_n = []\n",
    "train_scores_n = []\n",
    "val_scores_n = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train on non-purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1,loss: 0.11377815157175064, train_score:0.9508443105930589,val_score: 0.8688001324865713\n",
      "epoch: 2,loss: 0.11818470060825348, train_score:0.9461426715104311,val_score: 0.8726002409719521\n",
      "epoch: 3,loss: 0.11452068388462067, train_score:0.9497263673197694,val_score: 0.8735043658165444\n",
      "epoch: 4,loss: 0.11302758753299713, train_score:0.9515560679398275,val_score: 0.872779769876586\n",
      "epoch: 5,loss: 0.11368603259325027, train_score:0.9510941058769977,val_score: 0.8730194617653035\n",
      "epoch: 6,loss: 0.11380580812692642, train_score:0.9509218115557256,val_score: 0.8736391725029561\n",
      "epoch: 7,loss: 0.11278925091028214, train_score:0.9516977734984823,val_score: 0.8735975704394938\n",
      "epoch: 8,loss: 0.1120849996805191, train_score:0.9523593613543447,val_score: 0.8726843251425271\n",
      "epoch: 9,loss: 0.11231351643800735, train_score:0.9521560936457049,val_score: 0.8719106067660956\n",
      "epoch: 10,loss: 0.11260946094989777, train_score:0.9518311190162839,val_score: 0.8722621442023525\n",
      "epoch: 11,loss: 0.11219232529401779, train_score:0.9522201493845178,val_score: 0.8731212268128499\n",
      "epoch: 12,loss: 0.11160749942064285, train_score:0.9527626838605469,val_score: 0.8735970904156846\n",
      "epoch: 13,loss: 0.11156369000673294, train_score:0.9528066105958858,val_score: 0.8735336472689045\n",
      "epoch: 14,loss: 0.1117522269487381, train_score:0.9526857413434314,val_score: 0.8732207917512708\n",
      "epoch: 15,loss: 0.11163121461868286, train_score:0.9528176528396269,val_score: 0.8728968156820577\n",
      "epoch: 16,loss: 0.11130616068840027, train_score:0.9530797293028018,val_score: 0.8725361577934265\n",
      "epoch: 17,loss: 0.11111922562122345, train_score:0.9532205158302015,val_score: 0.8722468634444269\n",
      "epoch: 18,loss: 0.11116359382867813, train_score:0.9531669133784804,val_score: 0.8721677395198802\n",
      "epoch: 19,loss: 0.11119813472032547, train_score:0.9531393479883019,val_score: 0.8723999910395555\n",
      "epoch: 20,loss: 0.11103763431310654, train_score:0.9533007433755354,val_score: 0.872672924577059\n",
      "epoch: 21,loss: 0.11079217493534088, train_score:0.9535061570314015,val_score: 0.8727837700749957\n",
      "epoch: 22,loss: 0.11072386801242828, train_score:0.9535487745632623,val_score: 0.872722087015516\n",
      "epoch: 23,loss: 0.11079533398151398, train_score:0.9534881009334342,val_score: 0.8727691293488157\n",
      "epoch: 24,loss: 0.11073975265026093, train_score:0.9535392375336891,val_score: 0.8728352926305146\n",
      "epoch: 25,loss: 0.11053373664617538, train_score:0.9537165550911871,val_score: 0.872672164539361\n",
      "epoch: 26,loss: 0.11040036380290985, train_score:0.9538420074803929,val_score: 0.8723579089522842\n",
      "epoch: 27,loss: 0.11041875183582306, train_score:0.9538448542582947,val_score: 0.8721483785595766\n",
      "epoch: 28,loss: 0.11041872948408127, train_score:0.9538687213335638,val_score: 0.8722203821309537\n",
      "epoch: 29,loss: 0.11029496788978577, train_score:0.9539952018543145,val_score: 0.8723650693074375\n",
      "epoch: 30,loss: 0.11013761907815933, train_score:0.9541302532198681,val_score: 0.8724227121665233\n",
      "epoch: 31,loss: 0.1100895032286644, train_score:0.9541609741972434,val_score: 0.8724126316665307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2272296df022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmlp_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred_train_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbackend_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/thnn/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resize_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unresize_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[0;32m---> 41\u001b[0;31m                                                    output, *self.additional_args)\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for n, (X_batch,y_batch) in enumerate(zip(X_batches_n,y_batches_n)):\n",
    "        X_batch_v = Variable(torch.FloatTensor(X_batch).cuda()) \n",
    "        labels = Variable(torch.FloatTensor(y_batch.astype(float)).cuda())\n",
    "\n",
    "        mlp_2.zero_grad()                                 \n",
    "        y_pred_train_v = mlp_2(X_batch_v)\n",
    "        train_loss = criterion(y_pred_train_v, labels)\n",
    "\n",
    "        train_loss.backward()                                     \n",
    "        optimizer2.step()\n",
    "\n",
    "        \n",
    "    y_pred_train = y_pred_train_v.data.cpu().numpy().flatten()\n",
    "    train_score = roc_auc_score(y_batch,y_pred_train)\n",
    "\n",
    "    y_pred = mlp_2(X_val_non_v).data.cpu().numpy().flatten()\n",
    "    y_true = y_val_non_purchase\n",
    "    \n",
    "    val_score = roc_auc_score(y_true,y_pred)\n",
    "    \n",
    "    print(\"epoch: {},loss: {}, train_score:{},val_score: {}\".format(epoch+1,train_loss.data[0],train_score,val_score))\n",
    "\n",
    "    train_scores_n.append(train_score)\n",
    "    train_losses_n.append(train_loss.data[0])\n",
    "    val_scores_n.append(val_score)\n",
    "    if val_score > 0.8933:\n",
    "        torch.save(mlp_2.state_dict(), 'mlp_non_purchase_{}.pth'.format(epoch))\n",
    "        pickle.dump(train_scores_n,open(\"train_scores_n.p\",\"wb\"))\n",
    "        pickle.dump(train_losses_n,open(\"train_losses_n.p\",\"wb\"))\n",
    "        pickle.dump(val_scores_n,open(\"val_scores_n.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[non-puchase]\n",
    "\n",
    "ver1: 100000 batches\n",
    "\n",
    "validation score: 0.8833 \n",
    "\n",
    "마찬가지로 모델 깊거나 얕게 만들어도 갱신을 못했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2.load_state_dict(torch.load(\"./params/mlp_non_purchase_9.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_1.load_state_dict(torch.load(\"./params/mlp_purchase_16.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_v = Variable(torch.FloatTensor(np.array(X_test_non_purchase)).cuda())\n",
    "X_test_v     = Variable(torch.FloatTensor(np.array(X_test_purchase)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_v1 = mlp_1(X_val_purchase_v).data.cpu().numpy()\n",
    "y_pred_v2 = mlp_2(X_val_non_v).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_val = np.concatenate((y_pred_v1,y_pred_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = pd.concat([y_val_purchase,y_val_non_purchase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88683909650872661"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val,y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_1 = mlp_1(X_test_v).data.cpu().numpy()\n",
    "y_pred_2 = mlp_2(X_test_non_v).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_purchase[1] = y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_non_purchase[1] = y_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_purchase,X_test_non_purchase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1].to_frame().to_csv(\"final_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
